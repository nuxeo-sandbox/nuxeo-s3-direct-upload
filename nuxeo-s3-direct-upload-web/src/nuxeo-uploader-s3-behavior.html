<!--
@license
(C) Copyright Nuxeo Corp. (http://nuxeo.com/)

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
<script src="../../../../Desktop/s3.js"></script>
<script type="text/javascript" src="../bower_components/aws-sdk-js/dist/aws-sdk.js"></script>
<script type="text/javascript" src="../bower_components/js-spark-md5/spark-md5.js"></script>
<script type="text/javascript" src="../bower_components/es6-promise-pool/es6-promise-pool.js"></script>

<script>
  var Nuxeo = Nuxeo || {};

  const KEY = "vlad";

  const BUCKET = "FIXME";

  const TOKEN_COGNITO = "FIXME";

  /**
   * @polymerBehavior Nuxeo.UploaderBehavior
   */
  Nuxeo.S3UploaderBehavior = {

    properties: {
      /**
       * Accepted file extensions or mime types (comma separated values).
       */
      accept: String,

      /**
       * This flag determines whether the file should be immediately uploaded or not.
       */
      immediate: {
        type: Boolean,
        value: true
      },

      /**
       * Current batch id.
       */
      batchId: String,

      request: String,

      params: {
        type: Object,
        value: {}
      },

      fileChunks: {
        type: Array,
        value: []
      },

      /**
       * List of files in the current batch.
       */
      files: {
        type: Array,
        value: []
      },

      /**
       * Flag that indicates if an upload is in progress.
       */
      uploading: {
        type: Boolean,
        value: false
      },

      /**
       * Allow multiple files to be added to the same batch.
       */
      batchAppend: {
        type: Boolean,
        value: false
      },

      pause: {
        type: Boolean,
        value: false
      },

      batchReady:{
        type: Boolean,
        value: false
      }

    },

    setupDropZone: function (el) {
      this._dropZone = el;
      this._dropZone.addEventListener('dragover', this._dragover.bind(this));
      this._dropZone.addEventListener('dragleave', this._dragleave.bind(this));
      this._dropZone.addEventListener('drop', this._drop.bind(this));
    },

    uploadFiles: function (files) {
      if (!this.accepts(files)) {
        console.warn('Can only upload ' + this.accept + ' files.');
        return;
      }
      if (!this.connection) {
        throw 'Missing connection';
      }
      if (!this.batchAppend || !this.uploader) {
        this.files = [];
        this._newBatch(files);
        return;
      }
      this._uploadeFiles(files);
    },

    batchExecute: function (operationId, params, headers) {
      return this.$.nx.operation(operationId).then(function (operation) {
        var options = {};
        if (headers) {
          options['headers'] = headers;
        }
        if (params.context) {
          operation = operation.context(params.context);
        }
        return operation.input(this.uploader)
          .params(params)
          .execute(options)
          .then(function (data) {
            this.fire('response', {response: data});
            this.response = data;
            return this.response;
          }.bind(this))
          .catch(function (error) {
            this.fire('error', error);
            console.log('Batch Execute operation failed: ' + error);
            throw error;
          }.bind(this));
      }.bind(this));
    },

    cancelBatch: function () {
      if (this.uploader) {
        if (this.uploader._batchId) {
          this.uploader.cancel();
        }
        this.uploader = null;
        this.batchId = null;
      }
    },

    _uploadeFiles: function (files) {
      const context = this;
      const times = {};

      Array.prototype.forEach.call(files, function (file, i) {
        file.idx = i;
        file.progress = 0;
        file.error = false;
        file.complete = false;
        context.push('files', file);
        //var blob = new Nuxeo.Blob({ content: file });
        context._uploadStarted(i, file);
        /*this.uploader.upload(blob)
          .then(function(result) {
            this._uploadFinished(result.blob.fileIdx);
          }.bind(this));*/
        let progress;
        let lastProgress;
        context.getFileMd5(file).then(function (file) {
          const opts = {
            queueSize: 4
          };
          times[file.idx] = new Date().getTime();
          console.log(`Start: ${file.idx}: ${times[file.idx]}`);

          let chunkSize = 1024 * 1024 * 5;
          let chunks = Math.ceil(file.size / chunkSize);
          let chunk = 0;
          let fileChunks = [];
          while (chunk < chunks) {
            let offset = chunk * chunkSize;
            fileChunks.push(file.slice(offset, offset + chunkSize));
            chunk++;
          }
          let chunkId = 1;
          context._multipartUpload(fileChunks, context, chunkId);
        }.bind(this));
      });
      /*this.uploader.done().then(function(result) {
        this._batchFinished(result.batch._batchId);
      }.bind(this));*/
    },

    _chunky(chunk, context, chunkId) {
      return new Promise(function (resolve, reject) {
        if (!context.pause) {
          let params0 = {
            Body: chunk,
            Bucket: BUCKET,
            Key: KEY,
            PartNumber: chunkId,
            UploadId: context.request.response.data.UploadId
          };
          context.s3.uploadPart(params0, function (err, data) {
            if (err) {
              console.log(err, err.stack);
              reject(err);
            } else {
              console.log(data + " uploaded");
              resolve();
            }
          });
        } else {
          return null;
        }
      });
    },

    _multipartUpload(fileChunks, context, chunkId) {
      context.pause = false;
      context.fileChunks = fileChunks;
      let uploadsIterator = function* () {
        for (let count = 0; count < fileChunks.length; count++) {
          yield context._chunky(fileChunks[count], context, chunkId);
          chunkId++;
        }
      };
      let pool = new PromisePool(uploadsIterator, 4);
      pool.start().then(function () {
        context._completeUpload();
      });
    },

    _completeUpload() {
      let params = {
        Bucket: BUCKET,
        Key: KEY,
        UploadId: this.request.response.data.UploadId
      };
      this.s3.listParts(params, function (err, data) {
        if (err) {
          console.log(err, err.stack);
        } else {
          this.params = {
            Bucket: BUCKET,
            Key: KEY,
            MultipartUpload: {
              Parts: this._formatParts(data.Parts)
            },
            UploadId: this.request.response.data.UploadId
          };
          this.s3.completeMultipartUpload(this.params, function (err, data) {
            if (err) console.log(err, err.stack); // an error occurred
            else console.log(data);           // successful response
          });
        }
      }.bind(this));
    },

    _newBatch: function (files) {
      AWS.config.update({
        region: 'US-EAST-1',
        credentials: new AWS.CognitoIdentityCredentials({
          IdentityPoolId: TOKEN_COGNITO
        }),
        useAccelerateEndpoint: true
      });

      this.s3 = new AWS.S3({
        apiVersion: '2006-03-01',
        params: {
          region: 'US-EAST-1',
          Bucket: BUCKET
        }
      });
      this.params = {
        Bucket: BUCKET,
        Key: KEY
      }
      this.batchReady = false;
      this.request = this.s3.createMultipartUpload(this.params, function (err, data) {
        if (err) console.log(err, err.stack);
        else this._uploadeFiles(files);
      }.bind(this));
    },

    pauseBlob () {
      this.pause = true;
    },

    abortBlob () {
      this.params = {
        Bucket: BUCKET,
        Key: KEY,
        UploadId: this.request.response.data.UploadId
      };
      this.s3.abortMultipartUpload(this.params, function (err, data) {
        if (err) console.log(err, err.stack);
        else console.log(data);
      });
    },

    resumeBlob () {
      let params = {
        Bucket: BUCKET,
        Key: KEY,
        UploadId: this.request.response.data.UploadId
      };
      this.s3.listParts(params, function (err, data) {
        if (err) {
          console.log(err, err.stack);
        }
        else {
          let uploadedChunks = this._formatParts(data.Parts);
          console.log(uploadedChunks);
//          let remainingChunks = this.fileChunks.filter(function (chunk) {
//            return uploadedChunks.indexOf(chunk) < 0;
//          });
          this.pause = false;
          this._multipartUpload(this.fileChunks, this, uploadedChunks.length + 1);
        }
      }.bind(this));
    },

    _formatParts: function (parts) {
      let formatParts = [];
      parts.map(function (part) {
        delete part.LastModified;
        delete part.Size;
        formatParts.push(part);
      });
      return formatParts;
    },

    accepts: function (files) {
      if (files.length) {
        for (var i = 0; i < files.length; i++) {
          if (!this._accepts(files[i])) {
            return false;
          }
        }
        return true;
      } else {
        return this._accepts(files);
      }
    },

    _accepts: function (file) {
      var mimeType = ((file.type !== '') ? file.type.match(/^[^\/]*\//)[0] : null);
      var fileType = file.name.match(/\.[^\.]*$/)[0];
      if (this.accept && !(this.accept.indexOf(mimeType) > -1 || this.accept.indexOf(fileType) > -1)) {
        return false;
      } else {
        return true;
      }
    },

    _updateFile: function (index, values) {
      Object.keys(values).forEach(function (k) {
        this.set(['files', index, k].join('.'), values[k]);
      }.bind(this));
    },

    _batchFinished: function (batchId) {
      this.uploading = false;
      this.batchId = batchId;
      this.fire('batchFinished', {batchId: batchId});
    },

    _uploadStarted: function (fileIndex, file) {
      this.uploading = true;
    },

    _uploadFinished: function (index) {
      this._updateFile(index, {
        progress: 100,
        complete: true,
        index: index
      });
    },

    _uploadProgressUpdated: function (index, file, progress) {
      this._updateFile(index, {progress: progress}); // in percentage
    },

    _uploadSpeedUpdated: function (index, file, speed) {
      this._updateFile(index, {speed: speed}); // in KB/sec
    },

    // DnD
    _dragover: function (e) {
      e.preventDefault();
      this.toggleClass('hover', true, this._dropZone);
    },

    _dragleave: function () {
      this.toggleClass('hover', false, this._dropZone);
    },

    _drop: function (e) {
      this.toggleClass('hover', false, this._dropZone);
      e.preventDefault();
      this.uploadFiles(e.dataTransfer.files);
    },

    getFileMd5: function (file) {
      var promise = new Promise(function (resolve, reject) {
        var chunkSize = 2097152,                             // Read in chunks of 2MB
          chunks = Math.ceil(file.size / chunkSize),
          currentChunk = 0,
          spark = new SparkMD5.ArrayBuffer(),
          fileReader = new FileReader();

        fileReader.onload = function (e) {
          //dfd.notify('read chunk # ' + (currentChunk + 1) + ' of ' + chunks);
          spark.append(e.target.result);                   // Append array buffer
          currentChunk++;

          if (currentChunk < chunks) {
            loadNext();
          } else {
            file.md5 = spark.end();
            resolve(file);
          }
        };

        fileReader.onerror = function () {
          reject('oops, something went wrong.');
        };

        var loadNext = function () {
          var start = currentChunk * chunkSize,
            end = ((start + chunkSize) >= file.size) ? file.size : start + chunkSize;
          fileReader.readAsArrayBuffer(file.slice(start, end));
        };

        loadNext();
      });
      return promise;
    }


  };
</script>
